{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal Featurewise Attention Network (TFAN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temporal Featurewise Attention Network (TFAN) is an interpretable architecture for multi-variate time series forecast. This notebook can be used to build TFAN, train and evaluate the model on a given test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T14:02:25.579549Z",
     "start_time": "2021-04-02T14:02:25.549490Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.set_memory_growth(gpu, True)\n",
    "\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T09:25:38.504481Z",
     "start_time": "2021-04-02T09:25:38.474360Z"
    }
   },
   "outputs": [],
   "source": [
    "path = os.path.join(os.getcwd(), 'data')\n",
    "files = os.listdir(path)\n",
    "\n",
    "def load_data(path, list_of_files):\n",
    "    data = {}\n",
    "    for file in list_of_files:\n",
    "        filepath = os.path.join(path, file)\n",
    "        df = pd.read_cdv(filepath)\n",
    "        data[file] = df\n",
    "        print(f\"Loaded {file} with shape {data[file].shape}\")\n",
    "    return data\n",
    "\n",
    "# load data\n",
    "data = load_data(path, files)\n",
    "# concat data\n",
    "df = pd.concat([data[file] for file in data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T13:18:56.446825Z",
     "start_time": "2021-04-02T13:18:56.361089Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "# train-test-split\n",
    "df_train, df_test = train_test_split(df.to_numpy(), shuffle=False, train_size=0.8)\n",
    "\n",
    "# standardize\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df_train[:, :-1])\n",
    "x_train = scaler.transform(df_train[:, :-1])\n",
    "x_test = scaler.transform(df_test[:, :-1])\n",
    "\n",
    "df_train = np.concatenate((x_train, df_train[:, -1].reshape(-1, 1)), 1)\n",
    "df_test = np.concatenate((x_test, df_test[:, -1].reshape(-1, 1)), 1)\n",
    "\n",
    "# val set\n",
    "df_val, df_test = train_test_split(df_test, shuffle=False, train_size=0.5)   \n",
    "\n",
    "\n",
    "\n",
    "def create_dataset(df, window_size, batch_size, shuffle=False):\n",
    "    \"\"\" Create a tf.data.dataset from numpy array.\n",
    "    \"\"\"\n",
    "    data = df.astype('float32')\n",
    "    data = tf.data.Dataset.from_tensor_slices(data)\n",
    "    data = data.window(window_size, shift=1, stride=1, drop_remainder=True)\n",
    "    data = data.flat_map(lambda window: window.batch(window_size))\n",
    "    if shuffle:\n",
    "        data = data.shuffle(5000)\n",
    "    data = data.map(lambda k: (k[:,:-1], \n",
    "                               tf.reshape(k[:,-1], [window_size, 1]))) # tuple (x, y)\n",
    "    data = data.batch(batch_size, drop_remainder=True).prefetch(2)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T13:18:59.445351Z",
     "start_time": "2021-04-02T13:18:58.383169Z"
    }
   },
   "outputs": [],
   "source": [
    "from helpers.load_cfg import load_cfg\n",
    "\n",
    "cfg = load_cfg('cfg.txt')\n",
    "\n",
    "# inputs\n",
    "Tx = cfg['window_size'] # window size\n",
    "batch_size = cfg['batch_size']\n",
    "val_batch_size = cfg['val_batch_size']\n",
    "\n",
    "# create tf.datasets\n",
    "train_data = create_dataset(df_train, Tx, batch_size,\n",
    "                            shuffle=True)\n",
    "val_data = create_dataset(df_val, Tx, val_batch_size,\n",
    "                          shuffle=False)\n",
    "test_data = create_dataset(df_test, Tx, val_batch_size,\n",
    "                          shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T09:38:48.450273Z",
     "start_time": "2021-04-02T09:38:48.109104Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from tfan_layers import *\n",
    "\n",
    "# initialise model\n",
    "model = TFAN(residual_blocks=3, residual_dropout=0.2,\n",
    "             activation=tf.keras.activations.swish,\n",
    "             depthwise_padding=\"causal\", depthwise_kernel_size=2,\n",
    "             depth_multiplier=1, Tx=Tx,\n",
    "             kernel_initializer=tf.keras.initializers.HeUniform(),\n",
    "             num_heads=8, d_model=Tx,\n",
    "             regularization=\"dropout\", p=0.25,\n",
    "             final_filters=8, final_kernel_size=2,\n",
    "             final_dilations=[1, 2], final_padding='causal',\n",
    "             final_dropout=0.2)\n",
    "# build model\n",
    "num_features = df.shape[1] - 1 # columns - target -> adjust according to data\n",
    "model.build(input_shape=(batch_size, Tx, num_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T09:38:55.709460Z",
     "start_time": "2021-04-02T09:38:55.672725Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow_addons.optimizers import ExponentialCyclicalLearningRate\n",
    "\n",
    "# inputs\n",
    "maximal_learning_rate = 0.005\n",
    "initial_learning_rate = maximal_learning_rate * 0.1\n",
    "epoch_iterations = int(len(df_train)/batch_size)\n",
    "\n",
    "# early stopping\n",
    "checkpoint_filepath = os.path.join(os.getcwd(),'tmp', 'checkpoint')\n",
    "callbacks = [ tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", min_delta=0,\n",
    "                                               patience=5, verbose=1),\n",
    "             tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_filepath, save_weights_only=True,\n",
    "                                               monitor='val_loss', mode='min', save_best_only=True, \n",
    "                                                verbose=0)]  \n",
    "# Cyclic lr with exponential decay\n",
    "lr_schedule = ExponentialCyclicalLearningRate(\n",
    "    initial_learning_rate=initial_learning_rate,\n",
    "    maximal_learning_rate=maximal_learning_rate, gamma=0.97,\n",
    "    step_size = (epoch_iterations/2)) # step_size = 1/2 cycle\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=lr_schedule) \n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "# compile\n",
    "model.compile(optimizer=opt, loss=loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T11:27:23.544587Z",
     "start_time": "2021-04-02T09:39:01.491792Z"
    }
   },
   "outputs": [],
   "source": [
    "epochs = cfg['epochs']\n",
    "# fit model\n",
    "history = model.fit(train_data, validation_data=val_data, epochs=epochs, callbacks=callbacks) \n",
    "# load best model weights\n",
    "#model.load_weights(checkpoint_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T12:40:56.783995Z",
     "start_time": "2021-04-02T12:40:56.746058Z"
    }
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "\n",
    "# data\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epoch_stop = callbacks[0].stopped_epoch\n",
    "if epoch_stop == 0:\n",
    "    epoch_stop = epochs\n",
    "x_epochs = np.arange(1, epoch_stop+1)\n",
    "\n",
    "trace1 = go.Scatter(x=x_epochs,\n",
    "                    y=train_loss,\n",
    "                    mode=\"lines\",\n",
    "                    marker=dict(color = 'blue'),\n",
    "                    name='train loss')\n",
    "trace2 = go.Scatter(x=x_epochs,\n",
    "                    y=val_loss,\n",
    "                    mode=\"lines\",\n",
    "                    marker=dict(color = 'red'),\n",
    "                    name='validation loss')\n",
    "\n",
    "data = [trace1, trace2]\n",
    "layout = dict(title=\"train- and validation loss\",\n",
    "              yaxis=dict(title=\"Loss\", ticklen=5,zeroline=False),\n",
    "              xaxis=dict(title=\"Epochs\", ticklen=5,zeroline=False, \n",
    "                         tickmode='linear', tick0=1, dtick=1)\n",
    "             )\n",
    "fig = go.Figure(dict(data=data, layout=layout))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T13:19:28.819216Z",
     "start_time": "2021-04-02T13:19:06.206419Z"
    }
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "import numpy.ma as ma\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# inputs\n",
    "\n",
    "test = True # True : Test, False : Validation\n",
    "\n",
    "\n",
    "# create dataframes to store data when iterating over test sets\n",
    "\n",
    "if test:\n",
    "    iterator = iter(test_data)\n",
    "else:\n",
    "    iterator = iter(val_data)\n",
    "\n",
    "# iterate and get model predictions\n",
    "x, y = iterator.get_next()\n",
    "y_pred, _, _ = model(x, training=False)\n",
    "y_pred = y_pred[:, -1, :] # y_pred == shape(batch, 1)\n",
    "y_true = y[:, -1, :].numpy()\n",
    "\n",
    "# inverse transform features \n",
    "x = x[:, -1, :].numpy()\n",
    "x = scaler.inverse_transform(x)\n",
    "\n",
    "# build dataframe with features and store it in \"features\" dictionary\n",
    "cols = list(df.columns)[:-1]\n",
    "x_features = pd.DataFrame(x, columns=cols)\n",
    "\n",
    "# build dataframe with labels and predictions and store it in \"predictions\" dictionary\n",
    "cols = [\"Prediction\", \"True\"]\n",
    "predictions =  pd.DataFrame(np.concatenate(( y_pred, y_true ), axis=1), columns=cols)\n",
    "\n",
    "# Evaluate MSE\n",
    "\n",
    "mse = mean_squared_error(predictions[\"True\"], predictions[\"Prediction\"])\n",
    "print(f\"MSE: {mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T13:19:35.838924Z",
     "start_time": "2021-04-02T13:19:34.141879Z"
    }
   },
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as po\n",
    "\n",
    "# inputs \n",
    "\n",
    "start = 0\n",
    "end = 50000\n",
    "\n",
    "\n",
    "\n",
    "# select relevant window\n",
    "df = predictions[start:end]\n",
    "\n",
    "# True Target Values\n",
    "trace1 = go.Scatter(\n",
    "                    x=np.arange(start, end),\n",
    "                    y=df[\"True\"],\n",
    "                    mode=\"lines\",\n",
    "                    name=\"target\",\n",
    "                    marker=dict(color='rgb(20, 112, 204)'))\n",
    "# Predicted TFAN values\n",
    "trace2 = go.Scatter(\n",
    "                    x=np.arange(start,end),\n",
    "                    y=df[\"Prediction\"],\n",
    "                    mode=\"lines\",\n",
    "                    name=\"TFAN\",\n",
    "                    marker=dict(color='rgb(255, 55, 55)'))\n",
    "\n",
    "layout = dict(title=\"TFAN predictions on test trips - start: {} - end: {}\".format(start, end),\n",
    "              yaxis=dict(title=\"Target\", ticklen=5,zeroline= False),\n",
    "              xaxis=dict(title=\"Samples\", ticklen=5,zeroline= False)\n",
    "             )\n",
    "\n",
    "fig = go.Figure(data=[trace1, trace2], layout=layout)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate feature-wise attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The attention of the model in the context of time series-forecast is understood as the contribution to a certain prediction. This means that a high contribution reflects in high final attention weight and a lower contribution leads to a smaller attention weight. To do so, following components of MHA are studied:\n",
    "    * attention weights a\n",
    "    * norm of values ||v||\n",
    "    * norm of scaled values ||a*v||\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T13:20:10.198961Z",
     "start_time": "2021-04-02T13:19:57.444552Z"
    }
   },
   "outputs": [],
   "source": [
    "# The following tensors have the shapes:\n",
    "# y_pred == (batch, Tx, 1)\n",
    "# att == (batch, num_heads, features, features)\n",
    "# val == (batch, num_heads, features, depth) (Note: d_model = Tx = depth*num_heads)\n",
    "\n",
    "#inputs\n",
    "dataset = test_data\n",
    "# get data\n",
    "iterator = iter(dataset)\n",
    "x, y = iterator.get_next()\n",
    "\n",
    "# get tuple (predicitions, attention_weigths, values)\n",
    "y_pred, att_w, val = model(x, training=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T10:38:48.963970Z",
     "start_time": "2021-03-01T10:38:48.927423Z"
    }
   },
   "source": [
    "### Attention "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cells the notation will be:\n",
    "\n",
    "* ||v|| : the feature-wise norm of shape == (batch, num_heads, features) scaled by its Wo' component,\n",
    "    where Wo' is the segment of Wo (used to combine the different heads) applied to the specific head \n",
    "* a : the attention weights of shape == (batch, num_heads, features, features) \n",
    "* ||a*v|| : the norm of the feature-representations scaled by the attention weights\n",
    "\n",
    "Note that the result of the MHA is the result of the operation: x_mha = a@v. This means that every row of x_mha is computed by computing a different weighted sum of the values (each row is a different feature) with the attention weights.The weights in row m of the matrix a reflect how much the feature representations in v align with the specific feature (row  m in a). To give an example:\n",
    "\n",
    "The 3rd row of the matrix a (with one attention weight per feature) is used to perform a weighted sum of the values v, based on how much each of the features aligns with the 3rd feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-02T11:56:15.817528Z",
     "start_time": "2021-03-02T11:56:15.780911Z"
    }
   },
   "source": [
    "#### Scale values with Wo from linear combination of heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T13:20:22.362477Z",
     "start_time": "2021-04-02T13:20:22.149540Z"
    }
   },
   "outputs": [],
   "source": [
    "# get weights of linear layer used to combine the different heads of mha\n",
    "mha = model.layers[-2]\n",
    "batch_size = val.shape[0]\n",
    "num_heads = val.shape[1]\n",
    "depth = val.shape[-1]\n",
    "d_model = model.layers[-2].d_model\n",
    "if num_heads > 1:\n",
    "    wo = mha.weights[-2] # shape == (d_model, d_model) = (Tx, Tx) with d_model = num_heads*depth\n",
    "# note: that the bias is omitted as adding a fixed vector should not affect the inter-feature interactions\n",
    "\n",
    "# linearity of matrix multiplications is used to integrate the linear combination of heads into the values vector\n",
    "# A: scaled values B: attention weights C: values\n",
    "# A = B@C\n",
    "# A@Wo = (B@C)@Wo = B@(C@Wo)\n",
    "\n",
    "# compute v = v@Wo' for all heads\n",
    "\n",
    "# reshape Wo to shape == (1, num_heads, depth, d_model)\n",
    "if num_heads > 1:\n",
    "    wo = tf.reshape(wo, [1, num_heads, depth, d_model])\n",
    "    v = tf.matmul(val, wo) # shape == (batch, num_heads, features, d_model) \n",
    "else:\n",
    "    v = val # no linear combination is used to combine heads\n",
    "# Note: each feature representation of v now has d_model dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute ||v||, a and ||a*v||"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T13:20:30.761632Z",
     "start_time": "2021-04-02T13:20:24.463422Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_attention(att, values):\n",
    "    \"\"\" compute attention analysis using the attention weights att and values (scaled with Wo).\n",
    "    Args:\n",
    "        att: attention weighst of shape == (batch, num_heads, features, features)\n",
    "        values: values scaled with Wo of shape == (batch, num_heads, features, d_model)\n",
    "        \n",
    "    Output:\n",
    "        tuple (||v||, att, ||att*v||)\n",
    "    \"\"\"\n",
    "    # ||v||\n",
    "    v_norm = tf.norm(values, axis = -1) # (batch, num_heads, features)\n",
    "    # ||att*v||\n",
    "    # each row of att is used to perform a different weighted sum of the values\n",
    "    # thus, the attention weights need to be reshaped and expanded to element-wise multiply them with the values\n",
    "    att_expand = tf.transpose(att, perm = [0, 1, 3, 2])\n",
    "    att_expand = tf.expand_dims(att_expand, -2) # same weight along d_model dimension\n",
    "    values_expand = tf.expand_dims(values, -1) # use same set of values for all original rows of att\n",
    "    scaled_values = att_expand*values_expand # shape == (batch, num_heads, features, d_model, features)\n",
    "    scaled_values_norm = tf.norm(scaled_values, axis = -2) # shape == (batch, num_heads, features, features)\n",
    "    scaled_values_norm = tf.transpose(scaled_values_norm, perm = [0, 1, 3, 2]) # to have same alignment as att\n",
    "    # each row of scaled_values_norm has one value per feature, determining how to attend between the feature \n",
    "    # at the respective row and all input features\n",
    "            \n",
    "    return v_norm, att, scaled_values_norm\n",
    "\n",
    "# compute\n",
    "v_norm, att, scaled_values_norm = compute_attention(att_w, v)\n",
    "print(\"||v|| of shape == ({}, {}, {}) -> (batch size, num_heads, features)\".format(v_norm.shape[0], v_norm.shape[1], v_norm.shape[2]))\n",
    "print(\"att of shape == ({}, {}, {}, {}) -> (batch size, num_heads, features, features)\".format(att.shape[0], att.shape[1], att.shape[2], att.shape[3]))\n",
    "print(\"||att*v|| of shape == ({}, {}, {}, {}) -> (batch size, num_heads, features, features)\".format(scaled_values_norm.shape[0], scaled_values_norm.shape[1], \n",
    "                                                                                                      scaled_values_norm.shape[2], scaled_values_norm.shape[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-24T14:12:16.946652Z",
     "start_time": "2021-03-24T14:12:16.791931Z"
    }
   },
   "outputs": [],
   "source": [
    "from plots import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-04T12:15:54.274601Z",
     "start_time": "2021-03-04T12:15:54.246607Z"
    }
   },
   "source": [
    "#### Attention Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T12:57:37.848254Z",
     "start_time": "2021-04-01T12:57:27.588266Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, df_att = attention_distribution(att, name_of_features=[],\n",
    "                           start=None, end=None)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Featurewise Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = featurewise_attention(att, x_features, predictions['True'], predictions['Prediction'], feature=None,\n",
    "                            show_target=False, start=None, end=None)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
